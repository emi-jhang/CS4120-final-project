{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "445de4ff-1e31-4462-9e5d-9c3e8b6abf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/oliviagao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Download required NLTK data files\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Initialize SpaCy for tokenization and part of speech tagging\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba420a7-c04f-4e58-89b3-328e44dd9f02",
   "metadata": {},
   "source": [
    "## Loading in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f95b9a-60fd-4ed2-8da1-69c75a2f75b9",
   "metadata": {},
   "source": [
    "Data Description from Github:\n",
    "_______________________\n",
    "\n",
    "Word Complexity Lexicon (https://github.com/mounicam/lexical_simplification/tree/master/word_complexity_lexicon)\n",
    "_______________________\n",
    "\n",
    "\n",
    "lexicon.tsv : Each line consists of word and its complexity scores calculated by aggregating over human ratings. \n",
    "              The score belongs to a scale of 1-6, where 1 represents \"very simple\" and 6 represents \"very complex\"\n",
    "\n",
    "lexion_annotations.tsv: Each line consists of a word in the lexicon and its individual ratings from 11 annotators.\n",
    "                        Each rating again belongs to the scale of 1-6. -1 indicates that the annotator did not rate\n",
    "                        the word.\n",
    "                        \n",
    "NOTE: Both the files are tab delimited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4c78895-1cf0-40ca-b74d-545ae0893d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_data = pd.read_csv('../WCL_data/lexicon.csv')\n",
    "lexicon_ann = pd.read_csv('../WCL_data/lexicon_annotations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dfe4b0-b130-4ddb-afea-bc313d4242f1",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a0871f9-bc15-4edd-9c5f-1337dd702f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon data\n",
      "(15180, 2)\n",
      "            word  rating\n",
      "0            wet  1.5714\n",
      "1          cargo  2.8571\n",
      "2        Arsenal  3.7143\n",
      "3  Manufacturing  3.8333\n",
      "4           East  1.2857\n",
      "Lexicon ann\n",
      "(15180, 12)\n",
      "            word  ann_1  ann_2  ann_3  ann_4  ann_5  ann_6  ann_7  ann_8  \\\n",
      "0            wet     -1      1      1     -1      1      1      2      3   \n",
      "1          cargo     -1     -1      2     -1      4      2      2      4   \n",
      "2        Arsenal      4     -1     -1      4      5     -1      4      3   \n",
      "3  Manufacturing     -1      4      4     -1     -1      3      4      4   \n",
      "4           East     -1      2      1      1      1     -1      1      2   \n",
      "\n",
      "   ann_9  ann_10  ann_11  \n",
      "0     -1       2      -1  \n",
      "1     -1       3       3  \n",
      "2      3      -1       3  \n",
      "3     -1       4      -1  \n",
      "4      1      -1      -1  \n"
     ]
    }
   ],
   "source": [
    "# TODO: \n",
    "# Clean Data\n",
    "# split into train and test\n",
    "# Get dimensions, summary, etc. of data\n",
    "print(\"Lexicon data\")\n",
    "print(lexicon_data.shape)\n",
    "print(lexicon_data.head())\n",
    "\n",
    "print(\"Lexicon ann\")\n",
    "print(lexicon_ann.shape)\n",
    "print(lexicon_ann.head())\n",
    "\n",
    "data = lexicon_data.dropna(subset=[\"word\", \"rating\"])  # Drop rows with NaN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162adc7f-e0a1-4b4e-9d88-12abd3e5d88a",
   "metadata": {},
   "source": [
    "## Vectorizing, Regressor\n",
    "\n",
    "Training a regressor so that for words not present in the vocabulary, we can predict the difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0d1c172-9fed-4f30-9ac8-03ac83bbe423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3.250991774195391\n",
      "R-squared Score: -4.050382118710537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../WCL_regressor.pkl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "X = data[\"word\"].values  # Target words\n",
    "y = data[\"rating\"].values.astype(float)  # Continuous difficulty ratings\n",
    "\n",
    "# Vectorize words\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vectors = vectorizer.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectors, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train regressor\n",
    "regressor = RandomForestRegressor()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = regressor.predict(X_test)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R-squared Score:\", r2_score(y_test, y_pred))\n",
    "joblib.dump(regressor, \"../WCL_regressor.pkl\")\n",
    "# regressor = joblib.load(\"WCL_regressor.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274093f2-f286-44b1-9e68-9228530ac618",
   "metadata": {},
   "source": [
    "## Simplification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5917d904-d728-4720-a207-e874aebf5f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def simplify_sentence(sentence, regressor, vectorizer, word2vec_model, difficulty_threshold=3):\n",
    "    \"\"\"\n",
    "    Simplifies a sentence by replacing difficult words with simpler alternatives.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): Input sentence to be simplified.\n",
    "        regressor: Trained regressor model for predicting word difficulty.\n",
    "        vectorizer: Trained vectorizer for transforming words into features.\n",
    "        word2vec_model: Trained Word2Vec model for word similarity.\n",
    "        difficulty_threshold (float): Threshold above which words are considered difficult.\n",
    "    \n",
    "    Returns:\n",
    "        str: Simplified sentence.\n",
    "    \"\"\"\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    simplified_words = []\n",
    "    changed_words = []\n",
    "\n",
    "    for word in words:\n",
    "        # Check if the word exists in the difficulty dictionary (or predict difficulty if not)\n",
    "        vector = vectorizer.transform([word])\n",
    "        if word not in X:\n",
    "            difficulty = regressor.predict(vector)[0]  \n",
    "        else: \n",
    "            difficulty = data[data['word']==word]['rating'].to_numpy()\n",
    "        if difficulty > difficulty_threshold:\n",
    "            try:\n",
    "                similar_words = word2vec_model.wv.most_similar(word, topn=10)\n",
    "                print(similar_words)\n",
    "\n",
    "                for sim_word, _ in similar_words:\n",
    "                    sim_vector = vectorizer.transform([sim_word])\n",
    "                    sim_difficulty = regressor.predict(sim_vector)[0]\n",
    "\n",
    "                    if sim_difficulty <= difficulty:\n",
    "                        simplified_words.append(sim_word) \n",
    "                        changed_words.append((word, sim_word))\n",
    "                        break\n",
    "                    else:\n",
    "                        simplified_words.append(word)                                                            \n",
    "            except KeyError:\n",
    "                simplified_words.append(word)\n",
    "        else:\n",
    "            simplified_words.append(word)\n",
    "\n",
    "    simplified_sentence = \" \".join(simplified_words)\n",
    "    return simplified_sentence, changed_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad2ed1-5777-45c0-aca4-f0c9d905df1c",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "febf9da5-a1d8-4998-8b92-f054b54faef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: This is a simple sentence.\n",
      "Simplified Sentence: This is a simple sentences .\n",
      "Words Changed: [('sentence', 'sentences')] \n",
      "\n",
      "rouge1: Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "rouge2: Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "rougeL: Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "--------------------------------------------------\n",
      "Original Sentence: Although she was considered smart, she failed all her exams.\n",
      "Simplified Sentence: Although she was regarded smart , she failed all her exams .\n",
      "Words Changed: [('considered', 'regarded')] \n",
      "\n",
      "rouge1: Score(precision=0.9, recall=0.9, fmeasure=0.9)\n",
      "rouge2: Score(precision=0.7777777777777778, recall=0.7777777777777778, fmeasure=0.7777777777777778)\n",
      "rougeL: Score(precision=0.9, recall=0.9, fmeasure=0.9)\n",
      "--------------------------------------------------\n",
      "Original Sentence: Anachronism in historical contexts can be confusing.\n",
      "Simplified Sentence: Anachronism in historial context can be convoluted .\n",
      "Words Changed: [('historical', 'historial'), ('contexts', 'context'), ('confusing', 'convoluted')] \n",
      "\n",
      "rouge1: Score(precision=0.7142857142857143, recall=0.7142857142857143, fmeasure=0.7142857142857143)\n",
      "rouge2: Score(precision=0.5, recall=0.5, fmeasure=0.5)\n",
      "rougeL: Score(precision=0.7142857142857143, recall=0.7142857142857143, fmeasure=0.7142857142857143)\n",
      "--------------------------------------------------\n",
      "Original Sentence: accumulated, thesaurus, differing, terror\n",
      "Simplified Sentence: accumulating , dictionary , varying , terrorism\n",
      "Words Changed: [('accumulated', 'accumulating'), ('thesaurus', 'dictionary'), ('differing', 'varying'), ('terror', 'terrorism')] \n",
      "\n",
      "rouge1: Score(precision=0.5, recall=0.5, fmeasure=0.5)\n",
      "rouge2: Score(precision=0.0, recall=0.0, fmeasure=0.0)\n",
      "rougeL: Score(precision=0.5, recall=0.5, fmeasure=0.5)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "test_sentences = [\n",
    "    \"This is a simple sentence.\",\n",
    "    \"Although she was considered smart, she failed all her exams.\",\n",
    "    \"Anachronism in historical contexts can be confusing.\",\n",
    "    \"accumulated, thesaurus, differing, terror\"\n",
    "]\n",
    "for sentence in test_sentences:\n",
    "\n",
    "    simplified, changed_words = simplify_sentence(\n",
    "        sentence, \n",
    "        regressor, \n",
    "        vectorizer, \n",
    "        word2vec_model,  \n",
    "        difficulty_threshold=2\n",
    "    )\n",
    "    print(\"Original Sentence:\", sentence)\n",
    "    print(\"Simplified Sentence:\", simplified)\n",
    "    print(\"Words Changed:\", changed_words, \"\\n\")\n",
    "\n",
    "    scores = scorer.score(sentence, simplified)\n",
    "    for key in scores:\n",
    "        print(f'{key}: {scores[key]}')\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdcbe16-f84c-4b83-ad6b-37d051f363c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
