{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "445de4ff-1e31-4462-9e5d-9c3e8b6abf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emijhang/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "[nltk_data] Downloading package punkt to /Users/emijhang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Download required NLTK data files\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Initialize SpaCy for tokenization and part of speech tagging\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba420a7-c04f-4e58-89b3-328e44dd9f02",
   "metadata": {},
   "source": [
    "## Loading in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f95b9a-60fd-4ed2-8da1-69c75a2f75b9",
   "metadata": {},
   "source": [
    "Data Description from Github:\n",
    "_______________________\n",
    "\n",
    "Word Complexity Lexicon (https://github.com/mounicam/lexical_simplification/tree/master/word_complexity_lexicon)\n",
    "_______________________\n",
    "\n",
    "\n",
    "lexicon.tsv : Each line consists of word and its complexity scores calculated by aggregating over human ratings. \n",
    "              The score belongs to a scale of 1-6, where 1 represents \"very simple\" and 6 represents \"very complex\"\n",
    "\n",
    "lexion_annotations.tsv: Each line consists of a word in the lexicon and its individual ratings from 11 annotators.\n",
    "                        Each rating again belongs to the scale of 1-6. -1 indicates that the annotator did not rate\n",
    "                        the word.\n",
    "                        \n",
    "NOTE: Both the files are tab delimited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4c78895-1cf0-40ca-b74d-545ae0893d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_data = pd.read_csv('../WCL_data/lexicon.csv')\n",
    "lexicon_ann = pd.read_csv('../WCL_data/lexicon_annotations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dfe4b0-b130-4ddb-afea-bc313d4242f1",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a0871f9-bc15-4edd-9c5f-1337dd702f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon data\n",
      "(15180, 2)\n",
      "            word  rating\n",
      "0            wet  1.5714\n",
      "1          cargo  2.8571\n",
      "2        Arsenal  3.7143\n",
      "3  Manufacturing  3.8333\n",
      "4           East  1.2857\n",
      "Lexicon ann\n",
      "(15180, 12)\n",
      "            word  ann_1  ann_2  ann_3  ann_4  ann_5  ann_6  ann_7  ann_8  \\\n",
      "0            wet     -1      1      1     -1      1      1      2      3   \n",
      "1          cargo     -1     -1      2     -1      4      2      2      4   \n",
      "2        Arsenal      4     -1     -1      4      5     -1      4      3   \n",
      "3  Manufacturing     -1      4      4     -1     -1      3      4      4   \n",
      "4           East     -1      2      1      1      1     -1      1      2   \n",
      "\n",
      "   ann_9  ann_10  ann_11  \n",
      "0     -1       2      -1  \n",
      "1     -1       3       3  \n",
      "2      3      -1       3  \n",
      "3     -1       4      -1  \n",
      "4      1      -1      -1  \n"
     ]
    }
   ],
   "source": [
    "# TODO: \n",
    "# Clean Data\n",
    "# split into train and test\n",
    "# Get dimensions, summary, etc. of data\n",
    "print(\"Lexicon data\")\n",
    "print(lexicon_data.shape)\n",
    "print(lexicon_data.head())\n",
    "\n",
    "print(\"Lexicon ann\")\n",
    "print(lexicon_ann.shape)\n",
    "print(lexicon_ann.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d1c172-9fed-4f30-9ac8-03ac83bbe423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3.282869642779129\n",
      "R-squared Score: -4.099904058063263\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load and clean data\n",
    "data = lexicon_data.dropna(subset=[\"word\", \"rating\"])  # Drop rows with NaN\n",
    "\n",
    "X = data[\"word\"].values  # Target words\n",
    "y = data[\"rating\"].values.astype(float)  # Continuous difficulty ratings\n",
    "\n",
    "# Vectorize words\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vectors = vectorizer.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectors, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train regressor\n",
    "regressor = RandomForestRegressor()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = regressor.predict(X_test)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R-squared Score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5917d904-d728-4720-a207-e874aebf5f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def simplify_sentence(sentence, regressor, vectorizer, word2vec_model, difficulty_threshold=3):\n",
    "    \"\"\"\n",
    "    Simplifies a sentence by replacing difficult words with simpler alternatives.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): Input sentence to be simplified.\n",
    "        regressor: Trained regressor model for predicting word difficulty.\n",
    "        vectorizer: Trained vectorizer for transforming words into features.\n",
    "        word2vec_model: Trained Word2Vec model for word similarity.\n",
    "        difficulty_threshold (float): Threshold above which words are considered difficult.\n",
    "    \n",
    "    Returns:\n",
    "        str: Simplified sentence.\n",
    "    \"\"\"\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    simplified_words = []\n",
    "\n",
    "    # Loop over each word in the sentence\n",
    "    for word in words:\n",
    "        # Check if the word exists in the difficulty dictionary (or predict difficulty if not)\n",
    "        vector = vectorizer.transform([word])\n",
    "        if word not in X:\n",
    "            difficulty = regressor.predict(vector)[0]  # Predict difficulty of the word\n",
    "        else: \n",
    "            difficulty = data[data['word']==word]['rating'].to_numpy()\n",
    "        print(word)\n",
    "        print(difficulty)\n",
    "        if difficulty > difficulty_threshold:\n",
    "            # Word is considered difficult, try finding a simpler alternative\n",
    "            try:\n",
    "                # Get similar words using Word2Vec model\n",
    "                similar_words = word2vec_model.most_similar(word, topn=5)\n",
    "                print(similar_words)\n",
    "\n",
    "                # Try to find a similar word with a lower difficulty rating\n",
    "                for sim_word, _ in similar_words:\n",
    "                    sim_vector = vectorizer.transform([sim_word])\n",
    "                    sim_difficulty = regressor.predict(sim_vector)[0]\n",
    "\n",
    "                    if sim_difficulty <= difficulty_threshold:\n",
    "                        simplified_words.append(sim_word)  # Replace with simpler word\n",
    "                        break\n",
    "                else:\n",
    "                    # No suitable replacement found, keep the original word\n",
    "                    simplified_words.append(word)\n",
    "            except KeyError:\n",
    "                # Word not in Word2Vec vocabulary, keep the original word\n",
    "                simplified_words.append(word)\n",
    "        else:\n",
    "            # Word is not considered difficult, keep the original word\n",
    "            simplified_words.append(word)\n",
    "\n",
    "    # Join the simplified words to form the new sentence\n",
    "    simplified_sentence = \" \".join(simplified_words)\n",
    "    return simplified_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "febf9da5-a1d8-4998-8b92-f054b54faef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "1.1410666233766245\n",
      "is\n",
      "[1.]\n",
      "a\n",
      "[1.]\n",
      "simple\n",
      "[1.6667]\n",
      "sentence\n",
      "[2.3333]\n",
      ".\n",
      "1.07645651015651\n",
      "Original Sentence: This is a simple sentence.\n",
      "Simplified Sentence: This is a simple sentence .\n",
      "Although\n",
      "[2.]\n",
      "she\n",
      "[1.1429]\n",
      "was\n",
      "[1.4286]\n",
      "considered\n",
      "[3.1429]\n",
      "[('regarded', 0.7731536030769348), ('deemed', 0.6961521506309509), ('viewed', 0.6467924118041992), ('Considered', 0.6333076357841492), ('considers', 0.5919919610023499)]\n",
      "smart\n",
      "1.4231477655677651\n",
      ",\n",
      "1.07645651015651\n",
      "she\n",
      "[1.1429]\n",
      "failed\n",
      "[1.6667]\n",
      "all\n",
      "[1.2857]\n",
      "her\n",
      "[1.]\n",
      "exams\n",
      "[1.6667]\n",
      ".\n",
      "1.07645651015651\n",
      "Original Sentence: Although she was considered smart, she failed all her exams.\n",
      "Simplified Sentence: Although she was regarded smart , she failed all her exams .\n",
      "Anachronism\n",
      "1.07645651015651\n",
      "in\n",
      "[1.]\n",
      "historical\n",
      "[2.5]\n",
      "contexts\n",
      "[3.]\n",
      "[('context', 0.6575323939323425), ('meanings', 0.5609514713287354), ('subjectivities', 0.5602607131004333), ('frameworks', 0.5575310587882996), ('milieux', 0.5472158789634705)]\n",
      "can\n",
      "[1.]\n",
      "be\n",
      "[1.]\n",
      "confusing\n",
      "[2.8571]\n",
      "[('convoluted', 0.6691057085990906), ('confused', 0.6656284928321838), ('complicated', 0.6382964849472046), ('bewildering', 0.6178728938102722), ('Confusing', 0.598398745059967)]\n",
      ".\n",
      "1.07645651015651\n",
      "Original Sentence: Anachronism in historical contexts can be confusing.\n",
      "Simplified Sentence: Anachronism in historical context can be convoluted .\n",
      "accumulated\n",
      "[3.1429]\n",
      "[('accumulating', 0.7215311527252197), ('amassed', 0.7061579823493958), ('accumulate', 0.6496683359146118), ('accrued', 0.6343469619750977), ('amassing', 0.5654119849205017)]\n",
      ",\n",
      "1.07645651015651\n",
      "thesaurus\n",
      "3.0066094841824826\n",
      "[('dictionary', 0.6903666257858276), ('spellchecker', 0.582492470741272), ('Thesaurus', 0.5757037997245789), ('thesauruses', 0.5686792731285095), ('rhyming_dictionary', 0.5635773539543152)]\n",
      ",\n",
      "1.07645651015651\n",
      "differing\n",
      "[2.6667]\n",
      "[('varying', 0.802890419960022), ('divergent', 0.6994098424911499), ('conflicting', 0.686427891254425), ('Differing', 0.6729199290275574), ('vastly_differing', 0.669105052947998)]\n",
      ",\n",
      "1.07645651015651\n",
      "terror\n",
      "[2.1667]\n",
      "Original Sentence: accumulated, thesaurus, differing, terror\n",
      "Simplified Sentence: accumulating , dictionary , varying , terror\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load the Word2Vec model\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Sample sentence\n",
    "test_sentences = [\n",
    "    \"This is a simple sentence.\",\n",
    "    \"Although she was considered smart, she failed all her exams.\",\n",
    "    \"Anachronism in historical contexts can be confusing.\",\n",
    "    \"accumulated, thesaurus, differing, terror\"\n",
    "]\n",
    "for sentence in test_sentences:\n",
    "    \n",
    "    # Assuming you have defined or imported simplify_sentence, regressor, and vectorizer\n",
    "    simplified = simplify_sentence(\n",
    "        sentence, \n",
    "        regressor, \n",
    "        vectorizer, \n",
    "        word2vec_model,  # Use the model directly\n",
    "        difficulty_threshold=2.5\n",
    "    )\n",
    "    print(\"Original Sentence:\", sentence)\n",
    "    print(\"Simplified Sentence:\", simplified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5a724-0613-4015-8012-d281296647cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')  # Download additional multilingual WordNet data\n",
    "\n",
    "# import gensim.downloader as api\n",
    "# import nltk\n",
    "# from nltk.corpus import wordnet as wn\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # Load Word2Vec model\n",
    "# wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "# # Define a function to get WordNet synonyms\n",
    "# def get_synonyms(word):\n",
    "#     synonyms = set()\n",
    "#     for syn in wn.synsets(word):\n",
    "#         for lemma in syn.lemmas():\n",
    "#             synonyms.add(lemma.name())  # Add synonym lemma to the set\n",
    "#     return list(synonyms)\n",
    "\n",
    "# # Define a function to simplify the sentence\n",
    "# def simplify_sentence(sentence, model, difficulty_threshold=3):\n",
    "#     words = word_tokenize(sentence)\n",
    "#     simplified_words = []\n",
    "\n",
    "#     for word in words:\n",
    "#         # Skip punctuation and stopwords\n",
    "#         if word.isalnum():\n",
    "#             # Get Word2Vec vector for the word\n",
    "#             try:\n",
    "#                 word_vector = model[word.lower()]\n",
    "                \n",
    "#                 # If the word is deemed too difficult (based on length, etc.), replace it\n",
    "#                 if len(word) > difficulty_threshold:\n",
    "#                     synonyms = get_synonyms(word)\n",
    "                    \n",
    "#                     # Find the synonym with the most similar Word2Vec vector\n",
    "#                     best_synonym = None\n",
    "#                     max_similarity = -1\n",
    "#                     for syn in synonyms:\n",
    "#                         try:\n",
    "#                             # Compare the cosine similarity between word and synonym\n",
    "#                             similarity = model.similarity(word.lower(), syn)\n",
    "#                             if similarity > max_similarity:\n",
    "#                                 best_synonym = syn\n",
    "#                                 max_similarity = similarity\n",
    "#                         except KeyError:\n",
    "#                             continue\n",
    "\n",
    "#                     # If a good synonym is found, use it; otherwise, keep the original word\n",
    "#                     if best_synonym:\n",
    "#                         simplified_words.append(best_synonym)\n",
    "#                     else:\n",
    "#                         simplified_words.append(word)\n",
    "#                 else:\n",
    "#                     simplified_words.append(word)\n",
    "#             except KeyError:\n",
    "#                 # If the word is not in Word2Vec model, just keep the word\n",
    "#                 simplified_words.append(word)\n",
    "#         else:\n",
    "#             # For punctuation or non-alphanumeric, just keep it\n",
    "#             simplified_words.append(word)\n",
    "\n",
    "#     return ' '.join(simplified_words)\n",
    "\n",
    "# # Example sentence\n",
    "# sentence = \"Anachronism in historical contexts can be confusing.\"\n",
    "\n",
    "# # Simplify the sentence\n",
    "# simplified_sentence = simplify_sentence(sentence, wv)\n",
    "# print(\"Original Sentence:\", sentence)\n",
    "# print(\"Simplified Sentence:\", simplified_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171557dc-7ee5-41de-9f4b-9ad309a17fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
