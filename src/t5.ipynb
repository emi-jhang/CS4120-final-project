{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41ec1ab0-b7d0-4a7a-b729-408e60c2a24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/oliviagao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /Users/oliviagao/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from langdetect import detect\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "import torch\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import cmudict\n",
    "# Download the CMU Pronouncing Dictionary for syllable counting\n",
    "nltk.download('cmudict')\n",
    "d = cmudict.dict()\n",
    "\n",
    "# Load a dataset for text simplification\n",
    "dataset = load_dataset(\"bogdancazan/wikilarge-text-simplification\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dda7a202-6935-4d38-9385-0f9aa5f0f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(word):\n",
    "    \"\"\"Return the syllable count for a word.\"\"\"\n",
    "    word = word.lower()\n",
    "    if word in d:\n",
    "        return max([len(list(y for y in x if y[-1].isdigit())) for x in d[word]])  # Get the max syllables\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b1ed054-c33f-4af4-9222-227fe24981f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flesch_kincaid(text):\n",
    "    \"\"\"\n",
    "    Calculate Flesch Reading Ease and Flesch-Kincaid Grade Level for a given text.\n",
    "    \"\"\"\n",
    "    if text.strip() == '' or detect(text) != 'en':\n",
    "        return -1\n",
    "    \n",
    "    # Split text into sentences\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]  # Remove empty strings\n",
    "    num_sentences = len(sentences)\n",
    "\n",
    "    # Split text into words\n",
    "    words = re.findall(r'\\w+', text)\n",
    "    num_words = len(words)\n",
    "\n",
    "    # Count syllables in words\n",
    "    num_syllables = sum(syllable_count(word) for word in words)\n",
    "\n",
    "    # Calculate ASL and ASW\n",
    "    asl = num_words / num_sentences if num_sentences > 0 else 0\n",
    "    asw = num_syllables / num_words if num_words > 0 else 0\n",
    "\n",
    "    # Calculate Flesch Reading Ease\n",
    "    reading_ease = 206.835 - (1.015 * asl) - (84.6 * asw)\n",
    "\n",
    "    return reading_ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a861c64b-52b7-4d87-914b-ed97f2093835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': (148843, 2), 'validation': (494, 2), 'test': (191, 2)}\n",
      "there is manuscript evidence that austen continued to work on these pieces as late as the period and that her niece and nephew anna and james edward austen made further additions as late as.\n",
      "{'Normal': 'upper sorbian is a minority language spoken by sorbs in germany in the historical province of upper lusatia lrb hornja u ica in sorbian rrb which is today part of saxony.', 'Simple': 'there are around speakers of upper sorbian living in saxony. upper sorbian is a minority language in germany according to the european charter for regional or minority languages.'}\n",
      "{'Normal': 'his next work saturday follows an especially eventful day in the life of a successful neurosurgeon.', 'Simple': 'his next work at saturday will be a successful neurosurgeon.'}\n"
     ]
    }
   ],
   "source": [
    "# Print out some information about the dataset\n",
    "print(dataset.shape)\n",
    "print(dataset[\"train\"][0]['Normal'])\n",
    "print(dataset[\"validation\"][0])\n",
    "print(dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6315e3bf-7baa-4161-8f9c-37ccfb268e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Normal': 'there is manuscript evidence that austen continued to work on these pieces as late as the period and that her niece and nephew anna and james edward austen made further additions as late as.', 'Simple': 'there is some proof that austen continued to work on these pieces later in life. her nephew and niece james edward and anna austen may have made further additions to her work in around.', 'input_ids': [18356, 10, 132, 19, 14496, 2084, 24, 403, 324, 2925, 12, 161, 30, 175, 2161, 38, 1480, 38, 8, 1059, 11, 24, 160, 23642, 11, 23213, 3, 10878, 11, 7620, 15, 7, 3, 15, 26, 2239, 403, 324, 263, 856, 811, 7, 38, 1480, 38, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [132, 19, 128, 5594, 24, 403, 324, 2925, 12, 161, 30, 175, 2161, 865, 16, 280, 5, 160, 23213, 11, 23642, 7620, 15, 7, 3, 15, 26, 2239, 11, 3, 10878, 403, 324, 164, 43, 263, 856, 811, 7, 12, 160, 161, 16, 300, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'Normal': 'upper sorbian is a minority language spoken by sorbs in germany in the historical province of upper lusatia lrb hornja u ica in sorbian rrb which is today part of saxony.', 'Simple': 'there are around speakers of upper sorbian living in saxony. upper sorbian is a minority language in germany according to the european charter for regional or minority languages.', 'input_ids': [18356, 10, 4548, 78, 52, 12032, 19, 3, 9, 16392, 1612, 11518, 57, 3, 23651, 7, 16, 13692, 63, 16, 8, 4332, 7985, 13, 4548, 3, 7650, 144, 23, 9, 3, 40, 52, 115, 3, 6293, 1191, 3, 76, 3, 2617, 16, 78, 52, 12032, 3, 52, 52, 115, 84, 19, 469, 294, 13, 3, 7, 9, 226, 106, 63, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [132, 33, 300, 7215, 13, 4548, 78, 52, 12032, 840, 16, 3, 7, 9, 226, 106, 63, 5, 4548, 78, 52, 12032, 19, 3, 9, 16392, 1612, 16, 13692, 63, 1315, 12, 8, 14864, 13382, 21, 3518, 42, 16392, 8024, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'Normal': 'his next work saturday follows an especially eventful day in the life of a successful neurosurgeon.', 'Simple': 'his next work at saturday will be a successful neurosurgeon.', 'input_ids': [18356, 10, 112, 416, 161, 3, 7, 6010, 1135, 6963, 46, 902, 605, 1329, 239, 16, 8, 280, 13, 3, 9, 1574, 6567, 7, 19623, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [112, 416, 161, 44, 3, 7, 6010, 1135, 56, 36, 3, 9, 1574, 6567, 7, 19623, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Add the task prefix for T5 input format \n",
    "    inputs = [\"simplify: \" + text for text in examples['Normal']]\n",
    "    targets = examples['Simple']\n",
    "    \n",
    "    # Tokenize the inputs and outputs (text pairs)\n",
    "    model_inputs = tokenizer(inputs, padding=\"max_length\", max_length=MAX_LENGTH, truncation=True)\n",
    "    labels = tokenizer(targets, padding=\"max_length\", max_length=MAX_LENGTH, truncation=True)\n",
    "\n",
    "    # Add labels as tokenized targets (this will be used for decoder during training)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_train = filtered_train.map(tokenize_function, batched=True)\n",
    "tokenized_validation = filtered_validation.map(tokenize_function, batched=True)\n",
    "tokenized_test = filtered_test.map(tokenize_function, batched=True)\n",
    "\n",
    "print(tokenized_train[0])\n",
    "print(tokenized_validation[0])\n",
    "print(tokenized_test[0])\n",
    "\n",
    "train_subset = tokenized_train.shuffle(seed=42).select(range(2000))\n",
    "validation_subset = tokenized_validation.shuffle(seed=42).select(range(400))\n",
    "testing_subset = tokenized_test.shuffle(seed=42).select(range(191))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3873584d-0ed1-4ac4-a309-af2c1d2e7a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=1e-05, batch_size=8, epochs=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliviagao/myenv/lib/python3.10/site-packages/accelerate/accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/Users/oliviagao/myenv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 06:34, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.745400</td>\n",
       "      <td>0.725500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.096900</td>\n",
       "      <td>0.638859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=1e-05, batch_size=8, epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliviagao/myenv/lib/python3.10/site-packages/accelerate/accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/Users/oliviagao/myenv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 09:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.654000</td>\n",
       "      <td>0.596828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.604700</td>\n",
       "      <td>0.562396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.539200</td>\n",
       "      <td>0.549608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=1e-05, batch_size=16, epochs=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliviagao/myenv/lib/python3.10/site-packages/accelerate/accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/Users/oliviagao/myenv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 05:21, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.567600</td>\n",
       "      <td>0.526494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.579100</td>\n",
       "      <td>0.521082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=1e-05, batch_size=16, epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliviagao/myenv/lib/python3.10/site-packages/accelerate/accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/Users/oliviagao/myenv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 08:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.532200</td>\n",
       "      <td>0.499682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.539800</td>\n",
       "      <td>0.487785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.539600</td>\n",
       "      <td>0.484920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=3e-05, batch_size=8, epochs=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliviagao/myenv/lib/python3.10/site-packages/accelerate/accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/Users/oliviagao/myenv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 06:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.428700</td>\n",
       "      <td>0.419346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.418300</td>\n",
       "      <td>0.409564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=3e-05, batch_size=8, epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliviagao/myenv/lib/python3.10/site-packages/accelerate/accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/Users/oliviagao/myenv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 10:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.385900</td>\n",
       "      <td>0.398137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.392500</td>\n",
       "      <td>0.392723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.340600</td>\n",
       "      <td>0.391469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=3e-05, batch_size=16, epochs=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliviagao/myenv/lib/python3.10/site-packages/accelerate/accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/Users/oliviagao/myenv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 05:26, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.387800</td>\n",
       "      <td>0.390135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.408700</td>\n",
       "      <td>0.390030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=3e-05, batch_size=16, epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliviagao/myenv/lib/python3.10/site-packages/accelerate/accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/Users/oliviagao/myenv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 08:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.378900</td>\n",
       "      <td>0.389451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.405500</td>\n",
       "      <td>0.389032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.402400</td>\n",
       "      <td>0.388885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(model_name) \n",
    "\n",
    "learning_rates = [1e-5, 3e-5]\n",
    "batch_sizes = [8, 16]\n",
    "num_epochs = [2, 3]\n",
    "\n",
    "logs = []\n",
    "\n",
    "# Loop through all hyperparameter combinations\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for epochs in num_epochs:\n",
    "            print(f\"Training with lr={lr}, batch_size={batch_size}, epochs={epochs}\")\n",
    "            \n",
    "            # Set up training arguments\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=\"./results\",\n",
    "                eval_strategy=\"epoch\",\n",
    "                learning_rate=lr,\n",
    "                per_device_train_batch_size=batch_size,\n",
    "                per_device_eval_batch_size=8,\n",
    "                num_train_epochs=epochs,\n",
    "                weight_decay=0.01,\n",
    "                push_to_hub=False,\n",
    "                logging_dir='./logs',\n",
    "                logging_steps=10,\n",
    "                save_steps=10,\n",
    "                save_total_limit=2,\n",
    "                fp16=True,\n",
    "            )\n",
    "\n",
    "            # Set up Trainer\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_subset,\n",
    "                eval_dataset=validation_subset,\n",
    "            )\n",
    "\n",
    "            # Start training\n",
    "            trainer.train()\n",
    "            \n",
    "            logs.append(trainer.state.log_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5d11747-fe74-47cb-8bec-3b33b8ae46a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliviagao/myenv/lib/python3.10/site-packages/accelerate/accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/Users/oliviagao/myenv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 09:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.400600</td>\n",
       "      <td>0.405604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.399800</td>\n",
       "      <td>0.397824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.350200</td>\n",
       "      <td>0.396206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('../t5_tokenizer/tokenizer_config.json',\n",
       " '../t5_tokenizer/special_tokens_map.json',\n",
       " '../t5_tokenizer/spiece.model',\n",
       " '../t5_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Picked best hyperparameters: lr=3e-05, batch_size=8, epochs=3\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-05,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Set up Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=validation_subset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained('../t5_model')\n",
    "tokenizer.save_pretrained('../t5_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4140e9bb-3bb7-446b-8071-5c9abb01e771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The experiment yielded inconclusive results due to extraneous variables.\n",
      "Simplified: The experiment yielded inconclusive results due to extraneous variables.\n",
      "--------------------------------------------------\n",
      "Original: The ecosystem's biodiversity is crucial for maintaining ecological balance.\n",
      "Simplified: La biodiversité de l'écosystème est cruciale pour l'équilibre écologique.\n",
      "--------------------------------------------------\n",
      "Original: Although she was considered smart, she failed all her exams.\n",
      "Simplified: She was considered smart but she failed all her exams.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize rouge scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Set device to MPS (if available)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def simplify_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=50, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "test_sentences = [\n",
    "    \"The experiment yielded inconclusive results due to extraneous variables.\",\n",
    "    \"The ecosystem's biodiversity is crucial for maintaining ecological balance.\",\n",
    "    \"Although she was considered smart, she failed all her exams.\",\n",
    "]\n",
    "\n",
    "# Test sentences\n",
    "for sentence in test_sentences:\n",
    "    simplified = simplify_text(sentence)\n",
    "    print(f\"Original: {sentence}\")\n",
    "    print(f\"Simplified: {simplified}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3ddb52f-ff4c-4a2e-9d70-e71dd8ddc1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "rouge2: Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "rougeL: Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "Flesch Reading Ease: -18.50 -> -18.50\n",
      "rouge1: Score(precision=0.08333333333333333, recall=0.1, fmeasure=0.0909090909090909)\n",
      "rouge2: Score(precision=0.0, recall=0.0, fmeasure=0.0)\n",
      "rougeL: Score(precision=0.08333333333333333, recall=0.1, fmeasure=0.0909090909090909)\n",
      "Flesch Reading Ease: -23.27 -> -1.00\n",
      "rouge1: Score(precision=0.9, recall=0.9, fmeasure=0.9)\n",
      "rouge2: Score(precision=0.7777777777777778, recall=0.7777777777777778, fmeasure=0.7777777777777778)\n",
      "rougeL: Score(precision=0.9, recall=0.9, fmeasure=0.9)\n",
      "Flesch Reading Ease: 78.25 ->  86.71\n"
     ]
    }
   ],
   "source": [
    "# Print out evaluation metrics separately to not crowd the sentences\n",
    "sentences = [\"The experiment yielded inconclusive results due to extraneous variables.\", \"The ecosystem's biodiversity is crucial for maintaining ecological balance.\", \"Although she was considered smart, she failed all her exams.\"]\n",
    "simplified_sentences = [\"The experiment yielded inconclusive results due to extraneous variables.\", \"La biodiversité de l'écosystème est cruciale pour l'équilibre écologique.\", \"She was considered smart but she failed all her exams.\"]\n",
    "for sentence, simplified in zip(sentences, simplified_sentences):\n",
    "    scores = scorer.score(sentence, simplified)\n",
    "    for key in scores:\n",
    "        print(f'{key}: {scores[key]}') \n",
    "    reading_ease = flesch_kincaid(sentence)\n",
    "    simplified_reading_ease = flesch_kincaid(simplified)\n",
    "    print(f\"Flesch Reading Ease: {reading_ease:.2f} -> {simplified_reading_ease: .2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9354f4-bb9d-4585-a3ae-f95cfcde3d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
